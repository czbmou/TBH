# Auto-Encoding Twin-Bottleneck Hashing

本工作是基于**TBH**论文的复现工作，原论文和代码链接请点击[此处](https://github.com/ymcidence/TBH)。

## 工作环境

***

我们的运行环境以及要求的配置如下：

`python=3.9.13`

`tensorflow==2.16.1`

`keras==3.3.3`

## 复现工作

***

### 使用数据集：Cifar-10

本次复现工作我们使用了原作者给出的处理好Cifar-10数据集，该数据集是将原始数据利用CNN提取特征，再将其转换为tfRecords格式的数据。

在使用其他数据集时，我们尝试将其他数据集比如NUS-WIDE和COCO进行类似的格式转化操作，但经过长时间的尝试，都以失败告终。因此该复现工作仅在Cifar-10上进行。如果有其他处理好的数据集，可以按照原作者的方法在其他数据集上进行训练和测试。

### 基础复现工作

根据原论文思路以及相关参考代码，我们尝试自行构建TBH模型，最终完成了[tbh.py](./model/tbh.py)文件的编写，并在Cifar-10数据集上使用自行构建的TBH模型进行训练，在16bit、32bit和64bit都进行了训练和测试。同时还使用原作者提供的JMLH模型进行训练和测试，也得到了相应的运行结果。训练模型保存在了[result](./result)文件夹中。

### 代码模块改进

基于自行构建的TBH模型，我们尝试进行进一步的改进，改进的内容如下：

​	**· 在TBH类中，为判别器部分添加了全连接层**

​	**· 调整代码中build_adjacency_hamming部分的参数，将1.4调整为1.383**

当然，我们也在model文件夹下提供了改进前的TBH模型版本[tbh-previous.py](./model/tbh-previous.py)，该版本去掉了增加的全连接层以及恢复初始的参数设置。

### 改进结果

由于代码只测试了400个样例，得到的mAP值会略微低于原论文给出的结果，这是正常的。

当我们使用未修改的模型进行训练，得到在16bit下的测试集的mAP值为0.503。

*如果要使用其他编码位如32bit等等，可以修改tbh_train.py,run_tbh.py以及dataset.py文件下的对应位置*

在判别器部分增加了全连接层后，得到16bit下的测试集的mAP值为0.512。

在修改参数部分我们尝试了各种参数，如1.3、1.45、1.414、1.395、1.38等等，效果都不如原来参数为1.4的结果，最接近的一次设置参数为1.38得到的mAP值为0.502。当我们设置参数为1.383时得到了mAP值为0.5226，超过了0.512的得分。

上述代码模块的改进使得在部分测试集上得到的mAP值有所提升，推测在全部数据集上，最终得到的mAP值较原论文也会有所提升，但是由于时间和设备的制约，并未进行验证。

## 如何查看训练结果？

***

运行run_tbh.py后，训练的结果会保存在[result](./result)文件夹下，其中包含了log文件夹和model文件夹。我们使用tensorboard进行数据可视化。

首先确保安装了tensorboard插件，在终端输入如下指令查看当前安装的tensorboard版本：

`pip show tensorboard`

确保无误后进入存放log和model的文件夹中，输入指令：

`tensorboard --logdir=log`

如果你是使用绝对路径来访问，那么将log改为你的绝对路径即可：

`tensorboard --logdir=your path`

随后终端会输出一个网址，将其复制并在网页中打开，就能看到训练日志的可视化结果。

## 未来工作

***

本次复现工作对原始代码进行了一定的整合和修改，从而实现论文的复现工作。当然我们的工作并不完善，比如未在其他数据集上进行训练和测试，使用的测试集并非完整的测试集，以及对参数进行调整时只在16bit下进行测试。更多的复现和测试工作有待实现。